#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""

Import a Noark 5 extract into the API.

Test data can be found in
../noark5-validator-k/src/resources/test-uttrekk/uttrekk1/n5uttrekk

"""

from __future__ import print_function

__license__ = 'GNU General Public License v2 or later at users choice'
__author__ = 'Petter Reinholdtsen'

import sys
import os
sys.path.append(os.path.join(sys.path[0],'lib'))

import argparse
import json
import random
import string
import urllib2
from functools import partial
from lxml import etree

import n5core.endpoint
from n5core.endpoint import HTTPError

strukturns = 'http://www.arkivverket.no/standarder/noark5/arkivstruktur'

tagarkiv                 = '{%s}arkiv' % strukturns
tagarkivdel              = '{%s}arkivdel' % strukturns
tagarkivskaper           = '{%s}arkivskaper' % strukturns
tagklassifikasjonssystem = '{%s}klassifikasjonssystem' % strukturns
tagklasse                = '{%s}klasse' % strukturns
tagmappe                 = '{%s}mappe' % strukturns
tagmerknad               = '{%s}merknad' % strukturns
tagregistrering          = '{%s}registrering' % strukturns
tagpart                  = '{%s}part' % strukturns
tagforfatter             = '{%s}forfatter' % strukturns
tagkorrespondansepart    = '{%s}korrespondansepart' % strukturns
tagdokumentbeskrivelse   = '{%s}dokumentbeskrivelse' % strukturns
tagdokumentobjekt        = '{%s}dokumentobjekt' % strukturns

# List of fields to ignore during import until Nikita handle them correctly
excludes = {
    'arkiv': (
        'systemID',
    ),
    'arkivdel': (
        'systemID',
    ),
    'klassifikasjonssystem': (
        'systemID',
    ),
    'klasse': (
        'systemID',
        'noekkelord',
    ),
    'saksmappe' : (
        'systemID',
        'referanseArkivdel',
        'merknad',

        # FIXME these need to be created before using
        'administrativEnhet', 'saksansvarlig',
    ),
    'registrering': (
        'systemID',
        'noekkelord',
    ),
    'journalpost': (
        'systemID',
        'noekkelord',
        'avskrivning',
        'referanseArkivdel',
    ),
    'arkivnotat': (
        'systemID',
    ),
    'moeteregistrering': (
        'systemID',
        'noekkelord',
    ),
    'dokumentbeskrivelse' : (
        'systemID',
        'referanseArkivdel',
        'tilknyttetRegistreringSom', # FIXME missing in nikita?
    ),
    'dokumentobjekt': (
        'systemID',
        # Exclude values related to the files that are handled during
        # upload and not during dokumentobjekt creation.  Dropping
        # filestoerrelse might seem like a bad idea, but it is most
        # often a free text field that is hard to parse and incorrect
        # file size would also give a checksum mismatch and get
        # rejected anyway.  Thus incorrect file size would be detected
        # during upload that way.
        'referanseDokumentfil',
        'filstoerrelse',
    ),
}

metadata = {
    'arkiv': (
        'arkivstatus',
        'dokumentmedium',
    ),
    'arkivdel': (
        'arkivdelstatus',
        'dokumentmedium',
    ),
    'klassifikasjonssystem': (
        'klassifikasjonstype',
    ),
    'klasse': (
    ),
    'mappe': (
        'dokumentmedium',
#        'mappetype', # Denne mangler i arkivstruktur.xsd, kun i TG
    ),
    'saksmappe': (
        'dokumentmedium',
        'saksstatus',
    ),
    'registrering': (
        'dokumentmedium',
    ),
    'journalpost': (
        'dokumentmedium',
        'journalposttype',
        'journalstatus',
    ),
    'dokumentbeskrivelse': (
        'dokumenttype',
        'dokumentstatus',
        'dokumentmedium',
#       'tilknyttetRegistreringSom', # FIXME metadata catalog missing in nikita
    ),
    'dokumentobjekt': (
        'variantformat',
        'format',
    ),
    'korrespondansepartenhet': (
        'korrespondanseparttype',
    ),
    'korrespondansepartperson': (
        'korrespondanseparttype',
    ),
    'korrespondansepartintern': (
        'korrespondanseparttype',
    ),
    'partenhet': (
        'partrolle',
    ),
    'partperson': (
        'partrolle',
    ),
    'partintern': (
        'partrolle',
    ),
    'postadresse': (
#        'land',
#        'postnr',
#        'poststed',
    ),
}

default_timezone = "+01:00"

datefields = (
    'arkivertDato',
    'arkivperiodeSluttDato',
    'arkivperiodeStartDato',
    'avskrivningsdato',
    'avsluttetDato',
    'dokumentetsDato',
    'flytMottattDato',
    'flytSendtDato',
    'graderingsdato',
    'journaldato',
    'kassasjonsdato',
    'konvertertDato',
    'merknadsdato',
    'mottattDato',
    'nedgraderingsdato',
    'opprettetDato',
    'presedensDato',
    'presedensGodkjentDato',
    'saksdato',
    'skjermingOpphoererDato',
    'tilknyttetDato',
    'verifisertDato',
)

# Bad default values handed out from Nikita, values we remove from ny-* output
baddefaults = {
    'arkiv': (
        'beskrivelse',
    ),
    'arkivskaper': (
        'beskrivelse',
    ),
    'mappe': (
        'beskrivelse',
        'offentligTittel',
    ),
    'saksmappe': (
        'beskrivelse',
        'offentligTittel',
    ),
    'registrering': (
        'beskrivelse',
    ),
    'journalpost': (
        'beskrivelse',
    ),
}

# FIXME figure out a better way to handle formats?
formatmap = {
    'pdf':   ('fmt/14', 'application/pdf'), # actually PDF 1.0
    'pdf/a': ('fmt/95', 'application/pdf'), # actually PDF/A 1a
    'text':  ('x-fmt/111', 'text/plain'), # Plain text
}

def format2mime(format):
    if format in formatmap:
        return formatmap[format][1]
    else:
        return 'application/octet-stream'

class Importer(n5core.endpoint.Endpoint):
    def __init__(self, baseurl):
        self.nesting = 0
        self.dryrun = False
        n5core.endpoint.Endpoint.__init__(self, baseurl)
        self.stats = {}
        self.formats = {}
        self.metadatacache = {}

    def output(self, msg):
        print("%s %s" % ('=' * self.nesting, msg))

    def printstats(self):
        print("Imported entries and their count")
        for name in sorted(self.stats.keys()):
            print("%-25s %d" % (name, self.stats[name]))
        print()
        print("Imported formats and their count")
        for format in sorted(self.formats.keys()):
            print("%-25s %d" % (format, self.formats[format]))

    def metadata_make_up_unused_code(self, field, kodenavn):
        kodes = {}
        for v in self.metadatacache[field]:
            kodes[self.metadatacache[field][v]['kode']] = v
        bag = string.ascii_uppercase
        count = 10
        while 0 < count:
            candidate = ''.join(random.sample(bag, 2))
            if candidate not in kodes:
                return candidate
            count -= 1
        raise Exception("unable to find unused 'kode' value.")

    def metadatalookup(self, default, entity, field, verdi=None):
        kode = None
        kodenavn = verdi['kodenavn']
        if field in self.metadatacache:
            print("Cache: ", self.metadatacache[field])
            if kodenavn in self.metadatacache[field]:
                kode = self.metadatacache[field][kodenavn]['kode']
                return {
                    'kode': kode,
                    'kodenavn': kodenavn,
                }
            else:
                # Perhaps the value is a code already (like with format)?
                for v in self.metadatacache[field]:
                    if kode == self.metadatacache[field][v]['kode']:
                        return {
                            'kode': kodenavn,
                        }
        relkey = self.relbaseurl + 'metadata/%s/' % field

        # Prefer _links entries from ny-*, but fall back to searching
        # the entire API for the relevant metadata list.
        if 'links' in default and relkey in default['_links']:
            url = default['_links'][relkey]['href']
        else:
            print("error: unable to find %s in ny-%s, searching for it" % (relkey, entity))
            url = self.findRelation(relkey)
        if url:
            (c, r) = self.json_get(url)
            j = json.loads(c)
            if field not in self.metadatacache:
                self.metadatacache[field] = {}
            for v in j['results']:
                key = v['kodenavn']
                # No need for these
                if '_links' in v: del v['_links']
                if 'kodenavn' in v: del v['kodenavn']
                self.metadatacache[field][key] = v
                if v['kode'] == kodenavn:
                    kode = kodenavn
                    kodenavn = None
            if not kode and kodenavn in self.metadatacache[field]:
                kode = self.metadatacache[field][kodenavn]['kode']
        if not kode:
            print("info: trying to create new %s metadata catalog entry (%s,%s)" %(
                field, kode, kodenavn,
            ))
            newrelkey = self.nikitarelbaseurl + 'metadata/ny-%s/' % field
            url = self.findRelation(newrelkey)
            if url:
                kode = self.metadata_make_up_unused_code(field, kodenavn)
                data = {
                    'kode': kode,
                    'kodenavn': kodenavn,
                }
                print("info: POST %s: %s" % (url, data))
                (c, r) = self.json_post(url, data)
                j = json.loads(c)
                self.metadatacache[field][kodenavn] = kode
            else:
                raise Exception("unable to find url to register new %s metadata catalog entry %s" % (field, kodenavn))
        return {
            'kode': kode,
            'kodenavn': kodenavn,
        }

    # Copied from import-email
    def create_entity(self, name, rel, parent, data):
        if rel not in parent['_links'] or 'href' not in parent['_links'][rel]:
            if self.verbose:
                print(parent)
            raise Exception("unable to find %s in provided relations" % rel)
        url = parent['_links'][rel]['href']
        try:
            if self.verbose:
                print("GET %s" % url)
            (gc, gres) = self.json_get(url)
            default = json.loads(gc)
            # Transfer from ny-* JSON to POST data
            for k in default.keys():
                if not k == '_links' and k not in data:
                    # Skip bad default values
                    if name in baddefaults and k in baddefaults[name]:
                        continue
                    data[k] = default[k]
        except HTTPError as e:
            print("error: HTTP failure for %s: %s (%s)" % (
                url, str(e), e.response.content))
            pass
        try:
            # Map values if needed
            for field in data.keys():
                # Map metadata
                if name in metadata and field in metadata[name]:
                    value = self.metadatalookup(default, name,
                                                field, verdi=data[field])
                    data[field] = value
                # Map datestamps
                if field in datefields:
                    # Make sure the required time zone is in place
                    if -1 == data[field].find('+') and 'Z' != data[field][-1]:
                        print("warning: adding hardcoded timezone %s to %s.%s" %
                              (default_timezone, name, field))
                        data[field] = data[field] + default_timezone
            if self.verbose:
                print("POST: %s" % data)
            (c, res) = self.json_post(url, data)
            if name not in self.stats:
                self.stats[name] = 0
            self.stats[name] += 1
        except HTTPError as e:
            msg = e.read()
            print("Error: ", msg)
            raise
        info = json.loads(c)
        if self.verbose:
            # Validate the stuff we send came back after storing
            for f in data.keys():
                if data[f] is not None:
                    if  f not in info:
                        print("error: field %s=%s disappeared from object" %  (f, data[f]))
                    elif data[f] != info[f]:
                        print("error: field %s=%s do not match object value %s" %  (f, data[f], info[f]))
        return info

    def import_entity(self, name, rel, parentinfo, element, subs):
        self.nesting += 1
        data = {}
        for sub in element.iterchildren():
            if self.verbose:
                self.output("%s: '%s' - '%s'" % (name, sub.tag, sub.text))
            if sub.tag in subs:
                subs[sub.tag][0].append(sub)
            field = sub.tag.replace('{%s}' % strukturns, '')
            if None != sub.text:
                if name in excludes and field in excludes[name]:
                        self.output('ignoring %s field %s [%s]' % (name, field, sub.text))
                else:
                    if name in metadata and field in metadata[name]:
                        data[field] = {'kode': None, 'kodenavn': sub.text}
                    else:
                        data[field] = sub.text
        if self.verbose or self.dryrun:
            self.output("POST: %s" % data)
        if self.dryrun:
            info = None
        else:
            if self.verbose:
                print()
                print("Creating %s: %s" % (name, data))
                print()
            info = self.create_entity(name, rel, parentinfo, data)
        for tag in subs.keys():
            for sub in subs[tag][0]:
                subs[tag][1](info, sub)
        self.nesting -= 1
        return info

    def import_arkiv(self, parentinfo, element, sub=False):
        subs = {
            tagarkiv: ([], partial(self.import_arkiv, sub=True)),
            tagarkivskaper: ([], self.import_arkivskaper),
            tagarkivdel: ([], self.import_arkivdel),
        }
        rel = self.relbaseurl + 'arkivstruktur/ny-arkiv/'
        self.import_entity('arkiv', rel, parentinfo, element, subs)

    def import_arkivskaper(self, parentinfo, element):
        subs = {}
        rel = self.relbaseurl + 'arkivstruktur/ny-arkivskaper/'
        self.import_entity('arkivskaper', rel, parentinfo, element, subs)

    def import_arkivdel(self, parentinfo, element):
        subs = {
            tagmappe: ([], self.import_mappe),
            tagklassifikasjonssystem: ([], self.import_klassifikasjonssystem),
            tagregistrering: ([], self.import_registrering),
        }
        rel = self.relbaseurl + 'arkivstruktur/ny-arkivdel/'
        self.import_entity('arkivdel', rel, parentinfo, element, subs)

    def import_klassifikasjonssystem(self, parentinfo, element):
        subs = {
            tagklasse: ([], self.import_klasse),
        }
        rel = self.relbaseurl + 'arkivstruktur/ny-klassifikasjonssystem/'
        self.import_entity('klassifikasjonssystem', rel, parentinfo, element, subs)

    def import_klasse(self, parentinfo, element, sub=False):
        subs = {
            tagklasse: ([], partial(self.import_klasse, sub=True)),
            tagmappe: ([], self.import_mappe),
        }
        rel = self.relbaseurl + 'arkivstruktur/ny-klasse/'

        self.import_entity('klasse', rel, parentinfo, element, subs)

    def import_mappe(self, parentinfo, element, sub=False):
        attrtype = '{http://www.w3.org/2001/XMLSchema-instance}type'
        name = 'mappe'
        rel = self.relbaseurl + 'arkivstruktur/ny-mappe/'
        if attrtype in element.keys():
            etype = element.get(attrtype)
            if 'moetemappe' == etype:
                name = 'moetemappe'
                rel = self.relbaseurl + 'arkivstruktur/ny-moetemappe/'
            elif 'saksmappe' == etype:
                name = 'saksmappe'
                rel = self.relbaseurl + 'sakarkiv/ny-saksmappe/'
            else:
                raise ValueError("unknown %s type %s" % (element.tag, etype))
        subs = {
            tagmappe: ([], partial(self.import_mappe, sub=True) ),
            tagregistrering: ([], self.import_registrering),
#            tagmerknad: ([], self.import_merknad), # FIXME in mappe
#            tagpart: ([], self.import_part), # FIXME in mappe
        }
        self.import_entity(name, rel, parentinfo, element, subs)

    def import_registrering(self, parentinfo, element):
        name = 'registrering'
        rel = self.relbaseurl + 'arkivstruktur/ny-registrering/'
        attrtype = '{http://www.w3.org/2001/XMLSchema-instance}type'
        if attrtype in element.keys():
            etype = element.get(attrtype)
            if 'journalpost' == etype:
                name = 'journalpost'
                rel = self.relbaseurl + 'sakarkiv/ny-journalpost/'
            elif 'moeteregistrering' == etype:
                name = 'moeteregistrering'
                rel = self.relbaseurl + 'arkivstruktur/ny-moeteregistrering/'
            elif 'arkivnotat' == etype:
                name = 'arkivnotat'
                rel = self.relbaseurl + 'sakarkiv/ny-arkivnotat/'
            else:
                raise ValueError("unknown %s type %s" % (element.tag, etype))
        subs = {
            tagdokumentbeskrivelse: ([], self.import_dokumentbeskrivelse),
            tagkorrespondansepart: ([], self.import_korrespondansepart),
#            tagmerknad: ([], self.import_merknad), # FIXME in registrering
#            tagpart: ([], self.import_part), # FIXME in registrering
            tagforfatter: ([], self.import_forfatter),
        }
        self.import_entity(name, rel, parentinfo, element, subs)

    def import_merknad(self, parentinfo, element):
        subs = {}
        rel = self.relbaseurl + 'arkivstruktur/ny-merknad/'
        self.import_entity('merknad', rel, parentinfo, element, subs)

    def guess_part_type(self, data):
        """
Guess the part or korrespondansepart type to use.  Fall back to person if unsure.

administrativEnhet and saksbehandler indicates korrespondansepartintern
kontaktperson indikcates korrespondansepartenhet

use navn guessing to recognize entities (' AS', ' ASA') and people?

FIXME flag inaccurate guesses somehow
"""
        if 'administrativEnhet' in data or 'saksbehandler' in data:
            typename = 'intern'
        elif 'kontaktperson' in data \
           or -1 != data['navn'].find(' AS'):
            typename = 'enhet'
        else:
            typename = 'person'
        return typename


    def import_part(self, parentinfo, element):
        return self.import_generic_part('part', parentinfo, element)


    def import_korrespondansepart(self, parentinfo, element):
        return self.import_generic_part('korrespondansepart', parentinfo, element)


    def import_generic_part(self, basetype, parentinfo, element):
        fieldmap = {
            '%sNavn' % basetype: 'navn',
            'postadresse':            'postadresse.addresselinje1|addresselinje2|addresselinje3',
            'postnummer':             'postadresse.postnr',
            'poststed':               'postadresse.poststed',
            'land':                   'postadresse.landkode',
            'epostadresse':           'kontaktinformasjon.epostadresse',
            'telefonnummer':          'kontaktinformasjon.mobiltelefon|telefon',
            'partID':                 'systemID', # FIXME verify this mapping
        }
        data = {}
        for sub in element.iterchildren():
            field = sub.tag.replace('{%s}' % strukturns, '')
            value = sub.text
            if field in fieldmap:
                if -1 != fieldmap[field].find('.'):
                    d, t = fieldmap[field].split('.')
                    if d not in data:
                        data[d] = {}
                    if -1 != t.find('|'):
                        for f in t.split('|'):
                            if f not in data[d]:
                                data[d][f] = value
                                break
                    else:
                        data[d][t] = value
                else:
                    data[fieldmap[field]] = value
            else:
                data[field] = value

        # Norwegian mobile phone numbers are 4XX XX XXX or 9XX XX XXX
        # according to national number plan from NKom (E.164).
        if 'kontaktinformasjon' in data \
           and 'mobiltelefon' in data['kontaktinformasjon'] \
           and data['kontaktinformasjon']['mobiltelefon'][0] not in ('4', '9'):
            t = None
            if 'telefon' in data['kontaktinformasjon']:
                t = data['kontaktinformasjon']['telefon']
            data['kontaktinformasjon']['telefon'] = data['kontaktinformasjon']['mobiltelefon']
            if t:
                data['kontaktinformasjon']['mobiltelefon'] = t
            else:
                del data['kontaktinformasjon']['mobiltelefon']

        typename = self.guess_part_type(data)
        name = "%s%s" % (basetype, typename)
        rel = self.relbaseurl + 'arkivstruktur/ny-%s/' % name

        # FIXME no place for korrespondansepart.korrespondansepartNavn for 'intern'
        # FIXME perhaps create AdministrativEnhet for it?
        # FIXME no place for korrespondansepart.kontaktperson for 'intern'
        if 'intern' == typename:
            #del data['navn']
            #del data['kontaktperson']
            print("warning: dropping %s, not yet implemented in Nikita" % name)
            return

        if self.verbose:
            print()
            print("Creating %s: %s" % (name, data))
            print()
        if not self.dryrun:
            info = self.create_entity(name, rel, parentinfo, data)

    def import_dokumentbeskrivelse(self, parentinfo, element):
        subs = {
            tagdokumentobjekt: ([], self.import_dokumentobjekt),
#            tagpart: ([], self.import_part), # FIXME in dokumentbeskrivelse
            tagforfatter: ([], self.import_forfatter),
        }
        rel = self.relbaseurl + 'arkivstruktur/ny-dokumentbeskrivelse/'
        self.import_entity('dokumentbeskrivelse', rel, parentinfo, element, subs)

    def import_forfatter(self, parentinfo, element):
        rel = self.relbaseurl + 'arkivstruktur/ny-forfatter/'
        self.import_entity('forfatter', rel, parentinfo, element, None)

    def import_dokumentobjekt(self, parentinfo, element):
        rel = self.relbaseurl + 'arkivstruktur/ny-dokumentobjekt/'

        info = self.import_entity('dokumentobjekt', rel, parentinfo, element, {})

        if self.dryrun:
            return

        paths = list(element.iterchildren('{%s}referanseDokumentfil' % strukturns))
        if 1 < len(paths):
            raise ValueError('more than one referanseDokumentfil field is not allowed')
        filepath = os.path.join(self.basedir, paths[0].text)
        if '\\' in filepath:
            filepath = filepath.replace('\\', '/')
        try:
            with open(filepath) as content:
                size = os.path.getsize(filepath)
                # FIXME Workaround for bruken runtime and nikita 2020-01-13
                if 'format' in info:
                    mimetype = format2mime( info['format']['kodenavn'] )
                else:
                    mimetype = 'application/octet-stream'
                if self.verbose:
                    print("Uploading %s size %d mime type %s" % (filepath,
                                                                 size, mimetype))
                uploadrel = self.relbaseurl + 'arkivstruktur/fil/'
                if uploadrel not in info['_links'] or 'href' not in info['_links'][uploadrel]:
                    if self.verbose:
                        print(info)
                    raise Exception("missing from created dokumentobjekt: %s" % uploadrel)
                newfilehref = info['_links'][uploadrel]['href']
                if not self.dryrun:
                    try:
                        (c, res) = self.post(str(newfilehref), content,
                                             mimetype, length=size)
                    except HTTPError as e:
                        msg = e.read()
                        self.output('unable to POST to %s: %s' % (newfilehref, msg))
                        raise
        except IOError as e:
            self.output('unable to open %s' % filepath)
            raise
        if 'format' in info:
            formatcodename = info['format']['kodenavn']
        else:
            formatcodename = 'unknown'
        if formatcodename not in self.formats:
            self.formats[formatcodename] = 0
        self.formats[formatcodename] += 1

    def loaddir(self, basedir):
        self.basedir = basedir
        path = os.path.join(self.basedir, "arkivstruktur.xml")
        print(path)
        parser = etree.XMLParser(remove_blank_text=True)
        fh = open(path)
        tree = etree.parse(fh, parser)
        fh.close()
        element = tree.getroot()
        if element.tag == tagarkiv:
            (c, res) = self.json_get(self.findRelation(self.relbaseurl + 'arkivstruktur/'))
            parentinfo = json.loads(c)
            self.import_arkiv(parentinfo, element)
    #    print(etree.tostring(tree))

def main():
    baseurl = "https://localhost:8092/noark5v5/"
    
    parser = argparse.ArgumentParser()
    parser.add_argument("--baseurl", help="(default is %s)" % baseurl)
    parser.add_argument("--verbose", help="print more debug information",
                        action="store_true")
    parser.add_argument("--dryrun", help="print what would be imported, do not import",
                        action="store_true")
    parser.add_argument("basedir", nargs='+', help="directory with extract")
    args = parser.parse_args()

    if args.baseurl:
        baseurl = args.baseurl

    i = Importer(baseurl)
    i.verbose = args.verbose
    i.dryrun = args.dryrun
    i.login()

    retval = 0
    for basedir in args.basedir:
        i.loaddir(basedir)
    
    i.printstats()

    return retval

if __name__ == '__main__':
    exit(main())
